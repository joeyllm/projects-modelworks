# ModelWorks 

This repository contains two AI applications developed by the ModelWorks team:

## ğŸŒ¿ Branch Overview

### `main` - RAG-Powered Document Q&A System
A full-stack application that enables users to upload PDF documents and ask questions about their content using Retrieval-Augmented Generation (RAG) with local LLM models.

### `clinical-trials-chat-api` - Clinical Trial Analytics Dashboard
A Next.js application for clinical trial data visualization and AI-powered analysis of patient data, biomarkers, and trial outcomes.

## ğŸš€ Features

- **PDF Document Upload**: Drag-and-drop interface for uploading PDF files
- **Intelligent Q&A**: Ask questions about uploaded documents using RAG
- **Local LLM Integration**: Uses Ollama with DeepSeek R1 (1.5B) model
- **Vector Database**: ChromaDB for document storage and retrieval
- **Modern UI**: Responsive web interface with dark/light theme toggle
- **Real-time Chat**: Interactive chat interface for document queries
- **Dockerized**: Complete containerized setup for easy deployment

## ğŸ—ï¸ Architecture

The system consists of three main components:

- **Frontend**: HTML/CSS/JavaScript web interface
- **Backend**: FastAPI server with RAG pipeline
- **Ollama**: Local LLM service for text generation and embeddings

### Tech Stack

- **Backend**: FastAPI, Python, LangChain
- **Frontend**: HTML5, CSS3, JavaScript (Vanilla)
- **LLM**: Ollama with DeepSeek R1 (1.5B)
- **Embeddings**: Nomic Embed Text
- **Vector DB**: ChromaDB
- **PDF Processing**: pdfplumber
- **Containerization**: Docker & Docker Compose

## ğŸ“‹ Prerequisites

- Docker and Docker Compose
- Git

## ğŸš€ Quick Start

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd projects-modelworks
   ```

2. **Make the init script executable**:
   ```bash
   chmod +x init.sh
   ```

3. **Run the initialization script**:
   ```bash
   ./init.sh
   ```

This script will:
- Start all Docker containers
- Download the required LLM models (DeepSeek R1 1.5B and Nomic Embed Text)
- Set up the vector database

4. **Access the application**:
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:7860
   - Ollama: http://localhost:11434

## ğŸ“– Usage

1. **Upload a PDF**: Drag and drop a PDF file into the upload area
2. **Ask Questions**: Type your questions about the document content
3. **Get Answers**: The system will use RAG to provide contextually relevant answers

## ğŸ”§ API Endpoints

- `POST /upload`: Upload PDF files for processing
- `POST /ask`: Send questions about uploaded documents

## ğŸ“ Project Structure

```
â”œâ”€â”€ api/                    # API configuration
â”œâ”€â”€ backend/                # FastAPI backend
â”‚   â”œâ”€â”€ components/         # Core functionality modules
â”‚   â”‚   â”œâ”€â”€ database.py    # ChromaDB integration
â”‚   â”‚   â”œâ”€â”€ embedding.py   # Embedding model setup
â”‚   â”‚   â”œâ”€â”€ history.py     # Conversation history
â”‚   â”‚   â”œâ”€â”€ llm.py         # LLM integration
â”‚   â”‚   â”œâ”€â”€ pipeline.py    # RAG pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ retrieve.py    # Document retrieval
â”‚   â”‚   â”œâ”€â”€ store_text.py  # PDF text extraction
â”‚   â”‚   â””â”€â”€ upload.py      # File upload handling
â”‚   â”œâ”€â”€ api_handler.py     # FastAPI application
â”‚   â””â”€â”€ requirements.txt   # Python dependencies
â”œâ”€â”€ frontend/              # Web interface
â”‚   â”œâ”€â”€ index.html        # Main page
â”‚   â””â”€â”€ script.js         # Frontend logic
â”œâ”€â”€ docker-compose.yml     # Container orchestration
â””â”€â”€ init.sh               # Setup script
```

## ğŸ› ï¸ Development

### Backend Development

The backend uses a modular architecture with separate components for:

- **Pipeline**: Orchestrates the RAG workflow
- **Database**: Manages ChromaDB vector storage
- **LLM**: Handles text generation with Ollama
- **Embedding**: Manages text embeddings
- **Retrieve**: Implements similarity search
- **Upload**: Processes PDF files
- **History**: Manages conversation context

### Frontend Development

The frontend is built with vanilla JavaScript and provides:

- Drag-and-drop PDF upload
- Real-time chat interface
- Theme switching (dark/light mode)
- Responsive design

## ğŸ”’ Model Storage

Models are stored in a Docker volume (`ollama-data`) and persist across container restarts. To avoid losing models:

- âœ… Use `docker-compose down` (keeps volumes)
- âŒ Avoid `docker-compose down -v` (deletes volumes)
- âŒ Avoid `docker volume rm ollama-data`

## ğŸ› Troubleshooting

### Common Issues

1. **Models not downloading**: Ensure Ollama service is running and accessible
2. **Upload failures**: Check that the backend service is running on port 7860
3. **No responses**: Verify that the LLM model is properly loaded in Ollama

### Logs

Check container logs:
```bash
docker-compose logs backend
docker-compose logs ollama
```

---

## ğŸ§¬ Clinical Trials Branch (`clinical-trials-chat-api`)

### Overview
A sophisticated clinical trial analytics dashboard built with Next.js, featuring real-time patient data visualization, biomarker analysis, and AI-powered insights for the SCAI-001 immunotherapy study.

### Key Features

- **Patient Matching Algorithm**: Real-time patient enrollment and screening status
- **Biomarker Analysis**: EGFR mutations, PD-L1 expression, and ALK fusion analysis
- **Tumor Response Metrics**: Complete/Partial/Stable/Progressive disease tracking
- **Patient Outcomes**: Quality of Life (QoL) score monitoring and trends
- **Safety & Compliance**: Adverse events tracking and protocol deviation monitoring
- **AI Research Assistant**: GPT-4 powered chat interface for trial data analysis
- **Data Export**: PDF report generation and JSON data export

### Tech Stack

- **Frontend**: Next.js 15, React 18, TypeScript
- **Styling**: Tailwind CSS 4, Lucide React icons
- **Charts**: Recharts for data visualization
- **AI Integration**: OpenAI GPT-4 API
- **Data**: JSON-based patient dataset with 100+ mock patients

### Project Structure

```
app/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ chat/route.ts          # OpenAI API integration
â”‚   â””â”€â”€ data/                  # Patient datasets
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ClinicalTrialDemoMain.jsx  # Main dashboard component
â”‚   â”œâ”€â”€ Biomarkers.jsx         # Biomarker analysis
â”‚   â”œâ”€â”€ PatientMatching.jsx    # Patient enrollment
â”‚   â”œâ”€â”€ TumorMetrics.jsx      # Tumor response tracking
â”‚   â”œâ”€â”€ OutcomesSafety.jsx    # Safety monitoring
â”‚   â””â”€â”€ ChatDocsInterface.jsx  # AI chat interface
â”œâ”€â”€ globals.css               # Global styles
â”œâ”€â”€ layout.js                 # App layout
â””â”€â”€ page.jsx                  # Main page
```

### Key Features

#### 1. **Patient Matching Dashboard**
- Real-time enrollment status tracking
- Geographic distribution across Australian sites
- Screening pipeline management
- Eligibility criteria monitoring

#### 2. **Biomarker Analysis**
- EGFR mutation status visualization (67% positive)
- PD-L1 expression levels (47.8% average)
- ALK fusion analysis
- Interactive progress bars and charts

#### 3. **Tumor Response Tracking**
- RECIST criteria compliance
- Complete Response (CR), Partial Response (PR), Stable Disease (SD), Progressive Disease (PD)
- Tumor size change visualization
- Real-time metrics dashboard

#### 4. **Patient Outcomes & QoL**
- Baseline vs current Quality of Life scores
- Improvement tracking (+8.1 average improvement)
- Individual patient progress monitoring
- Trend analysis

#### 5. **Safety & Compliance**
- Severe adverse events tracking (Grade 3+)
- Protocol deviation monitoring
- Secondary infection tracking (COVID-19, Pneumonia, Sepsis)
- Compliance rate calculation (98.7%)

#### 6. **AI Research Assistant**
- GPT-4 powered chat interface
- Clinical trial data analysis
- Patient pattern recognition
- Research insights generation

### Setup Instructions

1. **Switch to the clinical trials branch**:
   ```bash
   git checkout clinical-trials-chat-api
   ```

2. **Install dependencies**:
   ```bash
   npm install
   ```

3. **Set up environment variables**:
   Create `.env.local`:
   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   ```

4. **Run the development server**:
   ```bash
   npm run dev
   ```

5. **Access the application**:
   Open [http://localhost:3000](http://localhost:3000)

### Data Structure

The application uses a comprehensive patient dataset with the following fields:

- **Demographics**: Age, sex, location
- **Clinical**: Cancer type, stage, enrollment status
- **Biomarkers**: EGFR mutation, PD-L1 expression, ALK fusion
- **Outcomes**: Tumor response, QoL scores, adverse events
- **Safety**: Protocol deviations, infections, compliance

### API Integration

- **OpenAI GPT-4**: For AI-powered clinical insights
- **Data Export**: PDF reports and JSON data downloads
- **Real-time Updates**: Live dashboard metrics

---
## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ‘¥ Team

- **Tom** â€“ Bachelor of Advanced Computing - Scribe
- **Xuan** â€“ Bachelor of Software Engineering â€“ Project Manager  
- **Jana** â€“ Bachelor of Software Engineering - Monitor
- **Arnav** â€“ Bachelor of Software Engineering - Spokesperson
- **Scarlett** â€“ Master of Computing - Checker
- **Josh** â€“ Bachelor of Computing - Deputy
- **Jaylee** â€“ Master of Computing - Coordinator

---
**ModelWorks Â© 2025** - Bridging AI research with practical applications
